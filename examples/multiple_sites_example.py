"""
ì—¬ëŸ¬ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜ˆì œ
URLì„ íŒŒë¼ë¯¸í„°ë¡œ ì €ì¥í•˜ê³  í•„ìš”í•  ë•Œ ì ‘ì†
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.crawler import NaverCrawler
from selenium.webdriver.common.by import By
import time


def example1_multiple_sites():
    """ì˜ˆì œ 1: ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ìˆœíšŒ í¬ë¡¤ë§"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 1: ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ìˆœíšŒ")
    print("="*60)
    
    # URL ë¦¬ìŠ¤íŠ¸
    urls = [
        "https://m.naver.com",
        "https://m.daum.net",
        "https://www.google.com",
    ]
    
    # í¬ë¡¤ëŸ¬ ìƒì„± (URLì€ ë‚˜ì¤‘ì— ì§€ì •)
    with NaverCrawler(use_mobile=True, headless=False) as crawler:
        for url in urls:
            print(f"\nğŸ“ ì ‘ì†: {url}")
            crawler.get_page(url)
            time.sleep(2)
            
            # ê°„ë‹¨í•œ í¬ë¡¤ë§
            crawler.scroll_down(300)
            time.sleep(1)
            
            print(f"âœ… {url} í¬ë¡¤ë§ ì™„ë£Œ")


def example2_initial_url():
    """ì˜ˆì œ 2: ì´ˆê¸° URL ì„¤ì • í›„ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 2: ì´ˆê¸° URL ì„¤ì •")
    print("="*60)
    
    # ì´ˆê¸° URLì„ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
    with NaverCrawler(url="https://m.naver.com", use_mobile=True, headless=False) as crawler:
        # ì´ˆê¸° URL ì ‘ì†
        print(f"ğŸ“ ì´ˆê¸° URL: {crawler.url}")
        crawler.get_page(crawler.url)
        time.sleep(2)
        
        # ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™
        print("ğŸ“ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
        crawler.get_page("https://m.daum.net")
        time.sleep(2)
        
        # ë˜ ë‹¤ë¥¸ í˜ì´ì§€
        print("ğŸ“ ë˜ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™")
        crawler.get_page("https://www.google.com")
        time.sleep(2)


def example3_blog_crawling():
    """ì˜ˆì œ 3: ì—¬ëŸ¬ ë¸”ë¡œê·¸ ê¸€ í¬ë¡¤ë§"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 3: ì—¬ëŸ¬ ë¸”ë¡œê·¸ ê¸€ í¬ë¡¤ë§")
    print("="*60)
    
    # ë¸”ë¡œê·¸ URL ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ)
    blog_urls = [
        "https://m.blog.naver.com",
        "https://m.blog.naver.com",
        "https://m.blog.naver.com",
    ]
    
    with NaverCrawler(use_mobile=True, device="galaxy_s24", headless=False) as crawler:
        for i, url in enumerate(blog_urls, 1):
            print(f"\nğŸ“– ë¸”ë¡œê·¸ {i}/{len(blog_urls)} ì ‘ì†: {url}")
            crawler.get_page(url)
            time.sleep(3)
            
            # ìì—°ìŠ¤ëŸ½ê²Œ ì½ê¸° (10~20ì´ˆ)
            print(f"   ì½ëŠ” ì¤‘...")
            crawler.simulate_natural_reading(min_read_time=10, max_read_time=20)
            
            print(f"âœ… ë¸”ë¡œê·¸ {i} ì™„ë£Œ")


def example4_search_multiple_keywords():
    """ì˜ˆì œ 4: ì—¬ëŸ¬ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 4: ì—¬ëŸ¬ í‚¤ì›Œë“œ ê²€ìƒ‰")
    print("="*60)
    
    keywords = ["íŒŒì´ì¬", "í¬ë¡¤ë§", "ì…€ë ˆë‹ˆì›€"]
    
    with NaverCrawler(url="https://m.naver.com", use_mobile=True, headless=False) as crawler:
        for keyword in keywords:
            print(f"\nğŸ” ê²€ìƒ‰: {keyword}")
            
            # ë„¤ì´ë²„ ë©”ì¸ìœ¼ë¡œ ì´ë™
            crawler.get_page(crawler.url)
            time.sleep(2)
            
            # ê²€ìƒ‰ì°½ ì°¾ì•„ì„œ ì…ë ¥
            if crawler.is_element_present(By.CSS_SELECTOR, "input.search_input"):
                search_box = crawler.find_element(By.CSS_SELECTOR, "input.search_input")
                search_box.clear()
                crawler.slow_typing(search_box, keyword)
                time.sleep(1)
                
                # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (ì‹¤ì œë¡œëŠ” ì—”í„°í‚¤ë‚˜ ë²„íŠ¼ í´ë¦­)
                # crawler.touch_element(By.CLASS_NAME, "search_btn")
                
                print(f"âœ… '{keyword}' ê²€ìƒ‰ ì™„ë£Œ")
            
            time.sleep(2)


def example5_category_pages():
    """ì˜ˆì œ 5: ì¹´í…Œê³ ë¦¬ë³„ í˜ì´ì§€ í¬ë¡¤ë§"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 5: ì¹´í…Œê³ ë¦¬ë³„ í˜ì´ì§€ í¬ë¡¤ë§")
    print("="*60)
    
    # ì¹´í…Œê³ ë¦¬ë³„ URL
    categories = {
        "ë‰´ìŠ¤": "https://m.news.naver.com",
        "ì‡¼í•‘": "https://shopping.naver.com",
        "ë¸”ë¡œê·¸": "https://section.blog.naver.com",
    }
    
    with NaverCrawler(use_mobile=True, headless=False) as crawler:
        for category_name, url in categories.items():
            print(f"\nğŸ“‚ {category_name} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§")
            print(f"   URL: {url}")
            
            crawler.get_page(url)
            time.sleep(3)
            
            # ìŠ¤í¬ë¡¤í•˜ë©° ë°ì´í„° ìˆ˜ì§‘
            print(f"   ìŠ¤í¬ë¡¤ ì¤‘...")
            crawler.dynamic_scroll(distance=800)
            time.sleep(1)
            
            # ì—¬ê¸°ì„œ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ì¶”ê°€
            # items = crawler.find_elements(By.CLASS_NAME, "item")
            
            print(f"âœ… {category_name} ì™„ë£Œ")


def example6_url_list_from_file():
    """ì˜ˆì œ 6: íŒŒì¼ì—ì„œ URL ëª©ë¡ ì½ì–´ì„œ í¬ë¡¤ë§"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 6: URL ëª©ë¡ íŒŒì¼ í¬ë¡¤ë§")
    print("="*60)
    
    # ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ê¸°
    # with open('urls.txt', 'r', encoding='utf-8') as f:
    #     urls = [line.strip() for line in f if line.strip()]
    
    # ì˜ˆì‹œ URL ëª©ë¡
    urls = [
        "https://m.naver.com",
        "https://m.daum.net",
        "https://www.google.com",
    ]
    
    print(f"ì´ {len(urls)}ê°œì˜ URL í¬ë¡¤ë§ ì˜ˆì •")
    
    with NaverCrawler(use_mobile=True, headless=False) as crawler:
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] {url}")
            
            try:
                crawler.get_page(url)
                time.sleep(2)
                
                # íƒ€ì´í‹€ ê°€ì ¸ì˜¤ê¸°
                title = crawler.driver.title
                print(f"   ì œëª©: {title}")
                
                # ìŠ¤í¬ë¡¤
                crawler.scroll_down(300)
                time.sleep(1)
                
                results.append({
                    "url": url,
                    "title": title,
                    "status": "success"
                })
                
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
                results.append({
                    "url": url,
                    "status": "failed",
                    "error": str(e)
                })
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        success = sum(1 for r in results if r['status'] == 'success')
        print(f"ì„±ê³µ: {success}/{len(urls)}")
        print(f"ì‹¤íŒ¨: {len(urls) - success}/{len(urls)}")


def example7_conditional_crawling():
    """ì˜ˆì œ 7: ì¡°ê±´ë¶€ í˜ì´ì§€ ì´ë™"""
    print("\n" + "="*60)
    print("ì˜ˆì œ 7: ì¡°ê±´ë¶€ í˜ì´ì§€ ì´ë™")
    print("="*60)
    
    with NaverCrawler(url="https://m.naver.com", use_mobile=True, headless=False) as crawler:
        # ë©”ì¸ í˜ì´ì§€ ì ‘ì†
        print("ğŸ“ ë„¤ì´ë²„ ë©”ì¸ ì ‘ì†")
        crawler.get_page(crawler.url)
        time.sleep(2)
        
        # íŠ¹ì • ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
        if crawler.is_element_present(By.CLASS_NAME, "news_area"):
            print("âœ… ë‰´ìŠ¤ ì˜ì—­ ë°œê²¬! ë‰´ìŠ¤ í˜ì´ì§€ë¡œ ì´ë™")
            crawler.get_page("https://m.news.naver.com")
            time.sleep(2)
            
            # ë‰´ìŠ¤ ìŠ¤í¬ë¡¤
            crawler.dynamic_scroll(distance=1000)
        else:
            print("â„¹ï¸ ë‰´ìŠ¤ ì˜ì—­ ì—†ìŒ. ë¸”ë¡œê·¸ë¡œ ì´ë™")
            crawler.get_page("https://section.blog.naver.com")
            time.sleep(2)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("ì—¬ëŸ¬ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜ˆì œ")
    print("="*80)
    
    examples = {
        '1': ('ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ìˆœíšŒ', example1_multiple_sites),
        '2': ('ì´ˆê¸° URL ì„¤ì •', example2_initial_url),
        '3': ('ì—¬ëŸ¬ ë¸”ë¡œê·¸ ê¸€ í¬ë¡¤ë§', example3_blog_crawling),
        '4': ('ì—¬ëŸ¬ í‚¤ì›Œë“œ ê²€ìƒ‰', example4_search_multiple_keywords),
        '5': ('ì¹´í…Œê³ ë¦¬ë³„ í˜ì´ì§€', example5_category_pages),
        '6': ('URL ëª©ë¡ íŒŒì¼ í¬ë¡¤ë§', example6_url_list_from_file),
        '7': ('ì¡°ê±´ë¶€ í˜ì´ì§€ ì´ë™', example7_conditional_crawling),
    }
    
    print("\nì‹¤í–‰í•  ì˜ˆì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for key, (name, _) in examples.items():
        print(f"{key}. {name}")
    print("0. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒ (0-7): ").strip()
    
    if choice == '0':
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    elif choice in examples:
        name, func = examples[choice]
        try:
            func()
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    print("\n" + "="*80)
    print("ì˜ˆì œ ì‹¤í–‰ ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    main()

