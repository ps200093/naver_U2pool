# URL íŒŒë¼ë¯¸í„° ì‚¬ìš© ê°€ì´ë“œ

`NaverCrawler`ì˜ `url` íŒŒë¼ë¯¸í„° ì‚¬ìš©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

`url` íŒŒë¼ë¯¸í„°ëŠ” **ì„ íƒì‚¬í•­**ì´ë©°, ì´ˆê¸° URLì„ ì €ì¥í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.  
ì‹¤ì œ í˜ì´ì§€ ì ‘ì†ì€ `get_page()` ë©”ì„œë“œë¡œ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ğŸ¯ ì‚¬ìš© ë°©ë²•

### 1. URL íŒŒë¼ë¯¸í„° ì—†ì´ ì‚¬ìš© (ê¸°ë³¸)

```python
from src.crawler import NaverCrawler

with NaverCrawler(use_mobile=True) as crawler:
    # ì›í•˜ëŠ” í˜ì´ì§€ë¡œ ì ‘ì†
    crawler.get_page("https://m.naver.com")
    crawler.scroll_down(500)
    
    # ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™
    crawler.get_page("https://m.daum.net")
    crawler.scroll_down(500)
```

### 2. URL íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜ ì‚¬ìš©

```python
from src.crawler import NaverCrawler

# ì´ˆê¸° URLì„ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ (ì €ì¥ë§Œ ë¨, ì ‘ì†ì€ ì•ˆë¨)
with NaverCrawler(url="https://m.naver.com", use_mobile=True) as crawler:
    # ì €ì¥ëœ URLë¡œ ì ‘ì†
    crawler.get_page(crawler.url)
    crawler.scroll_down(500)
    
    # ë‹¤ë¥¸ í˜ì´ì§€ë„ ììœ ë¡­ê²Œ ì ‘ì†
    crawler.get_page("https://m.daum.net")
```

### 3. ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ìˆœíšŒ

```python
# URL ë¦¬ìŠ¤íŠ¸
urls = [
    "https://m.naver.com",
    "https://m.daum.net",
    "https://www.google.com"
]

with NaverCrawler(use_mobile=True) as crawler:
    for url in urls:
        print(f"ì ‘ì†: {url}")
        crawler.get_page(url)
        crawler.dynamic_scroll(distance=800)
        time.sleep(2)
```

## ğŸ’¡ ì™œ ì´ë ‡ê²Œ ì„¤ê³„í–ˆë‚˜ìš”?

### ì¥ì 

1. **ìœ ì—°ì„±**: í•˜ë‚˜ì˜ í¬ë¡¤ëŸ¬ë¡œ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ ììœ ë¡­ê²Œ ë°©ë¬¸
2. **íš¨ìœ¨ì„±**: ë“œë¼ì´ë²„ë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©
3. **ëª…í™•ì„±**: ì–¸ì œ í˜ì´ì§€ì— ì ‘ì†í•˜ëŠ”ì§€ ëª…í™•í•˜ê²Œ ì œì–´

### ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

#### âœ… ì¢‹ì€ ì‚¬ìš© ì˜ˆ

```python
# ì—¬ëŸ¬ ë¸”ë¡œê·¸ ê¸€ í¬ë¡¤ë§
blog_urls = [
    "https://m.blog.naver.com/user1/123",
    "https://m.blog.naver.com/user2/456",
    "https://m.blog.naver.com/user3/789",
]

with NaverCrawler(use_mobile=True) as crawler:
    for url in blog_urls:
        crawler.get_page(url)
        # ìì—°ìŠ¤ëŸ½ê²Œ ì½ê¸°
        crawler.simulate_natural_reading(20, 40)
```

#### âœ… ì´ˆê¸° URL í™œìš©

```python
# ë©”ì¸ URLì„ ê¸°ì–µí•´ë‘ê³  í•„ìš”í•  ë•Œ ëŒì•„ê°€ê¸°
with NaverCrawler(url="https://m.naver.com") as crawler:
    # ë©”ì¸ìœ¼ë¡œ ì‹œì‘
    crawler.get_page(crawler.url)
    
    # ê²€ìƒ‰ ê²°ê³¼ë¡œ ì´ë™
    crawler.get_page("https://m.naver.com/search?query=...")
    
    # ë‹¤ì‹œ ë©”ì¸ìœ¼ë¡œ
    crawler.get_page(crawler.url)  # ì €ì¥ëœ URL ì¬ì‚¬ìš©
```

## ğŸ“š ì‹¤ì „ ì˜ˆì œ

### ì˜ˆì œ 1: ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§

```python
categories = {
    "ë‰´ìŠ¤": "https://m.news.naver.com",
    "ì‡¼í•‘": "https://shopping.naver.com",
    "ë¸”ë¡œê·¸": "https://section.blog.naver.com",
}

with NaverCrawler(use_mobile=True, device="galaxy_s24") as crawler:
    for name, url in categories.items():
        print(f"[{name}] í¬ë¡¤ë§ ì¤‘...")
        crawler.get_page(url)
        
        # ìŠ¤í¬ë¡¤í•˜ë©° ë°ì´í„° ìˆ˜ì§‘
        crawler.infinite_scroll(max_scrolls=5)
        
        # ë°ì´í„° ìˆ˜ì§‘ ë¡œì§
        items = crawler.find_elements(By.CLASS_NAME, "item")
        print(f"[{name}] {len(items)}ê°œ ì•„ì´í…œ ë°œê²¬")
```

### ì˜ˆì œ 2: ê²€ìƒ‰ í‚¤ì›Œë“œë³„ í¬ë¡¤ë§

```python
keywords = ["íŒŒì´ì¬", "í¬ë¡¤ë§", "ì…€ë ˆë‹ˆì›€"]

with NaverCrawler(url="https://m.naver.com", use_mobile=True) as crawler:
    for keyword in keywords:
        # ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸°
        crawler.get_page(crawler.url)
        time.sleep(1)
        
        # ê²€ìƒ‰
        search_box = crawler.find_element(By.CSS_SELECTOR, "input.search_input")
        search_box.clear()
        crawler.slow_typing(search_box, keyword)
        time.sleep(2)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        # ...
```

### ì˜ˆì œ 3: íŒŒì¼ì—ì„œ URL ëª©ë¡ ì½ê¸°

```python
# urls.txt íŒŒì¼ ë‚´ìš©:
# https://m.naver.com
# https://m.daum.net
# https://www.google.com

with open('urls.txt', 'r', encoding='utf-8') as f:
    urls = [line.strip() for line in f if line.strip()]

with NaverCrawler(use_mobile=True) as crawler:
    results = []
    
    for url in urls:
        try:
            crawler.get_page(url)
            title = crawler.driver.title
            
            results.append({
                "url": url,
                "title": title,
                "status": "success"
            })
        except Exception as e:
            results.append({
                "url": url,
                "status": "failed",
                "error": str(e)
            })
    
    # ê²°ê³¼ ì €ì¥
    from src.utils import save_to_json
    save_to_json(results, "crawl_results.json")
```

### ì˜ˆì œ 4: ì¡°ê±´ë¶€ í˜ì´ì§€ ì´ë™

```python
with NaverCrawler(url="https://m.naver.com", use_mobile=True) as crawler:
    # ë©”ì¸ í˜ì´ì§€
    crawler.get_page(crawler.url)
    
    # ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™
    if crawler.is_element_present(By.CLASS_NAME, "news_area"):
        print("ë‰´ìŠ¤ ì˜ì—­ ë°œê²¬ â†’ ë‰´ìŠ¤ í˜ì´ì§€ë¡œ")
        crawler.get_page("https://m.news.naver.com")
    elif crawler.is_element_present(By.CLASS_NAME, "shopping_area"):
        print("ì‡¼í•‘ ì˜ì—­ ë°œê²¬ â†’ ì‡¼í•‘ í˜ì´ì§€ë¡œ")
        crawler.get_page("https://shopping.naver.com")
    else:
        print("ê¸°ë³¸ ë¸”ë¡œê·¸ë¡œ")
        crawler.get_page("https://section.blog.naver.com")
```

## ğŸ”„ ê¸°ì¡´ ì½”ë“œì™€ì˜ ë¹„êµ

### ë³€ê²½ ì „ (ê¸°ì¡´)
```python
with NaverCrawler(use_mobile=True) as crawler:
    crawler.get_page("https://m.naver.com")
    # í¬ë¡¤ë§
```

### ë³€ê²½ í›„ (URL íŒŒë¼ë¯¸í„° ì¶”ê°€)
```python
# ë°©ë²• 1: ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³€ê²½ ì—†ìŒ)
with NaverCrawler(use_mobile=True) as crawler:
    crawler.get_page("https://m.naver.com")
    # í¬ë¡¤ë§

# ë°©ë²• 2: URL íŒŒë¼ë¯¸í„° í™œìš© (ì„ íƒì‚¬í•­)
with NaverCrawler(url="https://m.naver.com", use_mobile=True) as crawler:
    crawler.get_page(crawler.url)  # ì €ì¥ëœ URL ì‚¬ìš©
    # í¬ë¡¤ë§
```

**â†’ ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ë™ì‘í•©ë‹ˆë‹¤!**

## ğŸ“Œ ì£¼ìš” í¬ì¸íŠ¸

1. **`url` íŒŒë¼ë¯¸í„°ëŠ” ì„ íƒì‚¬í•­**: ì—†ì–´ë„ ì •ìƒ ì‘ë™
2. **ìë™ ì ‘ì† ì•ˆë¨**: `get_page()`ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì ‘ì†í•´ì•¼ í•¨
3. **ì—¬ëŸ¬ í˜ì´ì§€ ì ‘ì† ê°€ëŠ¥**: í•˜ë‚˜ì˜ í¬ë¡¤ëŸ¬ë¡œ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë°©ë¬¸
4. **ê¸°ì¡´ ì½”ë“œ í˜¸í™˜**: ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥

## ğŸ“ ë” ì•Œì•„ë³´ê¸°

- [examples/multiple_sites_example.py](../examples/multiple_sites_example.py) - ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ
- [ADVANCED_ACTIONS.md](./ADVANCED_ACTIONS.md) - ê³ ê¸‰ í¬ë¡¤ë§ ë™ì‘
- [README.md](../README.md) - ì „ì²´ ë¬¸ì„œ

---

**ì—…ë°ì´íŠ¸:** 2025-11-18  
**ë²„ì „:** 1.1.0

